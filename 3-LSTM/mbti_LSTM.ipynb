{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4SGJk_oF36x"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy\n",
        "!pip install gspread\n",
        "import gspread\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo3C0f_Cnwwb"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "!pip install tensorflow\n",
        "!pip install numpy\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **데이터 로드**"
      ],
      "metadata": {
        "id": "RtvbLpuYIvt_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q67CRhQ6n0KN"
      },
      "outputs": [],
      "source": [
        "data = ''  #데이터가 담긴 엑셀 파일의 구글 드라이브 경로"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSuGTnXE9l_v"
      },
      "outputs": [],
      "source": [
        "pd_data = pd.read_excel(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **텍스트 전처리**"
      ],
      "metadata": {
        "id": "QGBWpq8pH5a_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_jdRCqqMWmZ"
      },
      "outputs": [],
      "source": [
        "pd_data['내용'] = pd_data['내용'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\") \n",
        "pd_data['내용'] = pd_data['내용'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "pd_data['내용'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "pd_data = pd_data.dropna(subset = ['내용']).dropna(subset = ['e/i', 'n/s', 'f/t', 'p/j'], how = 'all')\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(pd_data))\n",
        "\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','을','으로','자','에','와','한','하다']\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "data = []\n",
        "for sentence in tqdm(pd_data['내용']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    data.append(stopwords_removed_sentence)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3fZhK6sXsDL"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YnqsqacXX1S"
      },
      "outputs": [],
      "source": [
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "tokenizer = Tokenizer(vocab_size) # 빈도수 3 미만인 단어는 제거\n",
        "tokenizer.fit_on_texts(data)\n",
        "data = tokenizer.texts_to_sequences(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfZoXYJ3XH2S"
      },
      "outputs": [],
      "source": [
        "data_s = data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **E/I 분류 데이터의 null 값 삭제 작업 및 텍스트의 길이 맞추기**"
      ],
      "metadata": {
        "id": "gssUzIK5IcLU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuiuCsXdzUGC"
      },
      "outputs": [],
      "source": [
        "data_y_ei = np.array(pd_data['e/i'])  ################   mbti의 4가지 카테고리에 따라 'e/i' 또는 'n/s' 또는 'f/t' 또는 'p/j'\n",
        "\n",
        "drop_ei = [index for index, sentence in enumerate(data_y_ei) if sentence != 0 and sentence != 1 ]\n",
        "\n",
        "data_ei = np.delete(data_s, drop_ei, axis=0)\n",
        "data_y_ei = np.delete(data_y_ei, drop_ei, axis=0)\n",
        "\n",
        "data_y_ei = data_y_ei.astype(float)\n",
        "\n",
        "drop_ei_1 = [index for index, sentence in enumerate(data_ei) if len(sentence) < 1]\n",
        "\n",
        "data_ei = np.delete(data_ei, drop_ei_1, axis=0)\n",
        "data_y_ei = np.delete(data_y_ei, drop_ei_1, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj9Cdd1uUK8d"
      },
      "outputs": [],
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
        "\n",
        "max_len = 100\n",
        "below_threshold_len(max_len, data_ei)\n",
        "\n",
        "# 전체 데이터의 길이는 100으로 맞춘다.\n",
        "data_ei = pad_sequences(data_ei, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N/S 분류 데이터의 null 값 삭제 작업 및 텍스트의 길이 맞추기**"
      ],
      "metadata": {
        "id": "qQjbMRyFI-6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ot8_zEZwQJDY"
      },
      "outputs": [],
      "source": [
        "data_y_ns = np.array(pd_data['n/s'])  ################   mbti의 4가지 카테고리에 따라 'e/i' 또는 'n/s' 또는 'f/t' 또는 'p/j'\n",
        "\n",
        "drop_ns = [index for index, sentence in enumerate(data_y_ns) if sentence != 0 and sentence != 1 ]\n",
        "\n",
        "data_ns = np.delete(data_s, drop_ns, axis=0)\n",
        "data_y_ns = np.delete(data_y_ns, drop_ns, axis=0)\n",
        "\n",
        "data_y_ns = data_y_ns.astype(float)\n",
        "\n",
        "drop_ns_1 = [index for index, sentence in enumerate(data_ns) if len(sentence) < 1]\n",
        "\n",
        "data_ns = np.delete(data_ns, drop_ns_1, axis=0)\n",
        "data_y_ns = np.delete(data_y_ns, drop_ns_1, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoRkMhdAQJDZ"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "below_threshold_len(max_len, data_ns)\n",
        "\n",
        "# 전체 데이터의 길이는 100으로 맞춘다.\n",
        "data_ns = pad_sequences(data_ns, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **F/T 분류 데이터의 null 값 삭제 작업 및 텍스트의 길이 맞추기**"
      ],
      "metadata": {
        "id": "E5CXI6KrJMNj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEqafjzvQ9XL"
      },
      "outputs": [],
      "source": [
        "data_y_ft = np.array(pd_data['f/t'])  ################   mbti의 4가지 카테고리에 따라 'e/i' 또는 'n/s' 또는 'f/t' 또는 'p/j'\n",
        "\n",
        "drop_ft = [index for index, sentence in enumerate(data_y_ft) if sentence != 0 and sentence != 1 ]\n",
        "\n",
        "data_ft = np.delete(data_s, drop_ft, axis=0)\n",
        "data_y_ft = np.delete(data_y_ft, drop_ft, axis=0)\n",
        "\n",
        "data_y_ft = data_y_ft.astype(float)\n",
        "\n",
        "drop_ft_1 = [index for index, sentence in enumerate(data_ft) if len(sentence) < 1]\n",
        "\n",
        "data_ft = np.delete(data_ft, drop_ft_1, axis=0)\n",
        "data_y_ft = np.delete(data_y_ft, drop_ft_1, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIbkjI7kQ9XM"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "below_threshold_len(max_len, data_ft)\n",
        "\n",
        "# 전체 데이터의 길이는 100으로 맞춘다.\n",
        "data_ft = pad_sequences(data_ft, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **P/J 분류 데이터의 null 값 삭제 작업 및 텍스트의 길이 맞추기**"
      ],
      "metadata": {
        "id": "ZUpBWrsAJVHj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYEotc3IRkla"
      },
      "outputs": [],
      "source": [
        "data_y_pj = np.array(pd_data['p/j'])  ################   mbti의 4가지 카테고리에 따라 'e/i' 또는 'n/s' 또는 'f/t' 또는 'p/j'\n",
        "\n",
        "drop_pj = [index for index, sentence in enumerate(data_y_pj) if sentence != 0 and sentence != 1 ]\n",
        "\n",
        "data_pj = np.delete(data_s, drop_pj, axis=0)\n",
        "data_y_pj = np.delete(data_y_pj, drop_pj, axis=0)\n",
        "\n",
        "data_y_pj = data_y_pj.astype(float)\n",
        "\n",
        "drop_pj_1 = [index for index, sentence in enumerate(data_pj) if len(sentence) < 1]\n",
        "\n",
        "data_pj = np.delete(data_pj, drop_pj_1, axis=0)\n",
        "data_y_pj = np.delete(data_y_pj, drop_pj_1, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJv_1ZcZRkla"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "below_threshold_len(max_len, data_pj)\n",
        "\n",
        "# 전체 데이터의 길이는 100으로 맞춘다.\n",
        "data_pj = pad_sequences(data_pj, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **E/I 분류 학습**"
      ],
      "metadata": {
        "id": "VWwSMdt9Jiuv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kND2vtbN8ngH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "es_ei = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
        "mc_ei = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_ei = Sequential()\n",
        "model_ei.add(Embedding(vocab_size, 100))\n",
        "model_ei.add(LSTM(128, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "model_ei.add(Dropout(0.5))\n",
        "model_ei.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0005)))\n",
        "\n",
        "opt = Adam(learning_rate=5e-4)\n",
        "model_ei.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model_ei.fit(data_ei, data_y_ei, epochs=15, callbacks=[es_ei, mc_ei], batch_size=64, validation_split=0.2)\n",
        "loaded_model_ei = load_model('best_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N/S 분류 학습**"
      ],
      "metadata": {
        "id": "wU50DD4nJl6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOIokajvP1Fo"
      },
      "outputs": [],
      "source": [
        "es_ns = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
        "mc_ns = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_ns = Sequential()\n",
        "model_ns.add(Embedding(vocab_size, 100))\n",
        "model_ns.add(LSTM(128, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "model_ns.add(Dropout(0.5))\n",
        "model_ns.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0005)))\n",
        "\n",
        "opt = Adam(learning_rate=5e-4)\n",
        "model_ns.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model_ns.fit(data_ns, data_y_ns, epochs=15, callbacks=[es_ns, mc_ns], batch_size=64, validation_split=0.2)\n",
        "loaded_model_ns = load_model('best_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **F/T 분류 학습**"
      ],
      "metadata": {
        "id": "xsZf9__vJs07"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo6rL7fRSIuK"
      },
      "outputs": [],
      "source": [
        "es_ft = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
        "mc_ft = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_ft = Sequential()\n",
        "model_ft.add(Embedding(vocab_size, 100))\n",
        "model_ft.add(LSTM(128, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "model_ft.add(Dropout(0.5))\n",
        "model_ft.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0005)))\n",
        "\n",
        "opt = Adam(learning_rate=5e-4)\n",
        "model_ft.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model_ft.fit(data_ft, data_y_ft, epochs=15, callbacks=[es_ft, mc_ft], batch_size=64, validation_split=0.2)\n",
        "loaded_model_ft = load_model('best_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **P/J 분류 학습**"
      ],
      "metadata": {
        "id": "F0TuZGcAJzWy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqSDpcUySV3W"
      },
      "outputs": [],
      "source": [
        "es_pj = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
        "mc_pj = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_pj = Sequential()\n",
        "model_pj.add(Embedding(vocab_size, 100))\n",
        "model_pj.add(LSTM(128, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "model_pj.add(Dropout(0.5))\n",
        "model_pj.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0005)))\n",
        "\n",
        "opt = Adam(learning_rate=5e-4)\n",
        "model_pj.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model_pj.fit(data_pj, data_y_pj, epochs=15, callbacks=[es_pj, mc_pj], batch_size=64, validation_split=0.2)\n",
        "loaded_model_pj = load_model('best_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MBTI 예측 함수 정의**"
      ],
      "metadata": {
        "id": "792HVOBmJ8cy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w8RjWQfsR7K"
      },
      "outputs": [],
      "source": [
        "def mbti_predict(new_sentence):\n",
        "  new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
        "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
        "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
        "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
        "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
        "\n",
        "  score_ei = float(loaded_model_ei.predict(pad_new)) # ei 예측\n",
        "  if(score_ei > 0.5):\n",
        "    mbti_ei = 'i'\n",
        "  else:\n",
        "    mbti_ei = 'e'\n",
        "\n",
        "  score_ns = float(loaded_model_ns.predict(pad_new)) # ns 예측\n",
        "  if(score_ns > 0.5):\n",
        "    mbti_ns = 's'\n",
        "  else:\n",
        "    mbti_ns = 'n'\n",
        "\n",
        "  score_ft = float(loaded_model_ft.predict(pad_new)) # ft 예측\n",
        "  if(score_ft > 0.5):\n",
        "    mbti_ft = 't'\n",
        "  else:\n",
        "    mbti_ft = 'f'\n",
        "\n",
        "  score_pj = float(loaded_model_pj.predict(pad_new)) # pj 예측\n",
        "  if(score_pj > 0.5):\n",
        "    mbti_pj = 'j'\n",
        "  else:\n",
        "    mbti_pj = 'p'\n",
        "\n",
        "  m = ['P','F','N','E','J','T','S','I']\n",
        "  ffff = pd.DataFrame({'P/J': [(1-score_pj)*100, score_pj*100], 'F/T': [(1-score_ft)*100, score_ft*100], 'N/S': [(1-score_ns)*100, score_ns*100], 'E/I': [(1-score_ei)*100, score_ei*100]}).T\n",
        "  ax = ffff.plot(kind = \"barh\", stacked= True, color = ('pink', 'skyblue'))\n",
        "  for p in range(len(ax.patches)):\n",
        "    left, bottom, width, height = ax.patches[p].get_bbox().bounds\n",
        "    ax.annotate(\" {e} : {:.2f}%\".format(width, e = m[p]), xy = (left+width/2, bottom+height/2), ha = 'center', va = 'center')\n",
        "  plt.box(False)  \n",
        "  ax.get_legend().remove()\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  plt.show()\n",
        "  print('\\033[1m'+'\\033[81m'+'\\n                  '+(mbti_ei.upper() + mbti_ns.upper() + mbti_ft.upper() + mbti_pj.upper())+ ' 입니다\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'+'\\033[0m')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MBTI를 예측할 텍스트 입력**"
      ],
      "metadata": {
        "id": "MWs2uuhsKG9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''나 게으르고 준비하는 거 귀찮고 나가는 것도 귀찮은데 \n",
        "막상 나가면 엄청 잘 놀아 ㅋㅋㅋㅋ \n",
        "애들이 내 텐션 따라가기 힘들대 ㅋㅋㅋㅋ \n",
        "근데 나랑 얘기하는 거 좋대 내가 공감을 잘 해줘서 ㅎㅎ \n",
        "친구들이 나한테 얘기 많이 털어놔. \n",
        "좀 귀찮긴 한데 같이 얘기하는 거 좋아 \n",
        "그러면 난 현실적인 조언 많이 해주면서도 공감도 해주니까 \n",
        "애들이 좋아하더라고\n",
        "물론 좀 귀찮긴 하더라'''"
      ],
      "metadata": {
        "id": "l-sFPNzTocdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MBTI 예측**"
      ],
      "metadata": {
        "id": "XHcsk72wKMoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mbti_predict(text)"
      ],
      "metadata": {
        "id": "nsvqEStAoeKK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "0603_github",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}