{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4SGJk_oF36x"
      },
      "outputs": [],
      "source": [
        "!pip install konlpy\n",
        "!pip install gspread\n",
        "import gspread\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo3C0f_Cnwwb"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install tqdm\n",
        "!pip install tensorflow\n",
        "!pip install numpy\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q67CRhQ6n0KN"
      },
      "outputs": [],
      "source": [
        "data = ' '  ################   수정해주세요!!  데이터가 담긴 엑셀 파일의 구글 드라이브 경로"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSuGTnXE9l_v"
      },
      "outputs": [],
      "source": [
        "##데이터 로드\n",
        "pd_data = pd.read_excel(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 7:3 비율로 train 데이터와 test 데이터를 나눈다. (shuffle)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(pd_data, test_size = 0.3,shuffle = True)"
      ],
      "metadata": {
        "id": "VayNkCfq6cCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_jdRCqqMWmZ"
      },
      "outputs": [],
      "source": [
        "## 전처리\n",
        "train_data['내용'] = train_data['내용'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\") \n",
        "train_data['내용'] = train_data['내용'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "train_data['내용'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "train_data = train_data.dropna(subset = ['내용']).dropna(subset = ['e/i', 'n/s', 'f/t', 'p/j'], how = 'all')\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(train_data))\n",
        "\n",
        "test_data['내용'] = test_data['내용'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\") \n",
        "test_data['내용'] = test_data['내용'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
        "test_data['내용'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
        "test_data = test_data.dropna(subset = ['내용']).dropna(subset = ['e/i', 'n/s', 'f/t', 'p/j'], how = 'all')\n",
        "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\n",
        "\n",
        "\n",
        "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','을','으로','자','에','와','한','하다']\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "X_train = []\n",
        "for sentence in tqdm(train_data['내용']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_train.append(stopwords_removed_sentence)\n",
        "\n",
        "X_test = []\n",
        "for sentence in tqdm(test_data['내용']):\n",
        "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
        "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
        "    X_test.append(stopwords_removed_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3fZhK6sXsDL"
      },
      "outputs": [],
      "source": [
        "## 단어 인덱스 형성\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "print(tokenizer.word_index)\n",
        "print(tokenizer.word_counts.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YnqsqacXX1S"
      },
      "outputs": [],
      "source": [
        "## 빈도수가 3회 미만인 단어는 삭제한다.\n",
        "threshold = 3\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "vocab_size = total_cnt - rare_cnt + 1\n",
        "tokenizer = Tokenizer(vocab_size) # 빈도수 3 미만인 단어는 제거\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfZoXYJ3XH2S"
      },
      "outputs": [],
      "source": [
        "X_train_s = X_train\n",
        "X_test_s = X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **E/I분류를 위한 null 값 삭제 작업**"
      ],
      "metadata": {
        "id": "YRW4x4zC2dsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ei = np.array(train_data['e/i'])  ################   mbti의 4가지 카테고리에 따라 'e/i' 또는 'n/s' 또는 'f/t' 또는 'p/j'\n",
        "y_test_ei = np.array(test_data['e/i'])   ################   mbti의 4가지 카테고리에 따라 'e/i' 또는 'n/s' 또는 'f/t' 또는 'p/j'"
      ],
      "metadata": {
        "id": "GZcNSx0y7GtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuiuCsXdzUGC"
      },
      "outputs": [],
      "source": [
        "## E/I분류를 위한 null 값 삭제 작업\n",
        "\n",
        "drop_train1_ei = [index for index, sentence in enumerate(y_train_ei) if sentence != 0 and sentence != 1 ]\n",
        "drop_test1_ei = [index for index, sentence in enumerate(y_test_ei) if sentence != 0 and sentence != 1 ]\n",
        "\n",
        "\n",
        "X_train_ei = np.delete(X_train_s, drop_train1_ei, axis=0)\n",
        "y_train_ei = np.delete(y_train_ei, drop_train1_ei, axis=0)\n",
        "\n",
        "X_test_ei = np.delete(X_test_s, drop_test1_ei, axis=0)\n",
        "y_test_ei = np.delete(y_test_ei, drop_test1_ei, axis=0)\n",
        "\n",
        "y_train_ei = y_train_ei.astype(float) \n",
        "y_test_ei = y_test_ei.astype(float)\n",
        "\n",
        "drop_train_ei = [index for index, sentence in enumerate(X_train_ei) if len(sentence) < 1]\n",
        "drop_test_ei = [index for index, sentence in enumerate(X_test_ei) if len(sentence) < 1]\n",
        "\n",
        "X_train_ei = np.delete(X_train_ei, drop_train_ei, axis=0)\n",
        "y_train_ei = np.delete(y_train_ei, drop_train_ei, axis=0)\n",
        "\n",
        "X_test_ei = np.delete(X_test_ei, drop_test_ei, axis=0)\n",
        "y_test_ei = np.delete(y_test_ei, drop_test_ei, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pj9Cdd1uUK8d"
      },
      "outputs": [],
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))\n",
        "\n",
        "max_len = 100\n",
        "below_threshold_len(max_len, X_train_ei)\n",
        "\n",
        "# 전체 데이터의 길이는 100으로 맞춘다.\n",
        "X_train_ei = pad_sequences(X_train_ei, maxlen = max_len)\n",
        "X_test_ei = pad_sequences(X_test_ei, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N/S분류를 위한 null 값 삭제 작업**"
      ],
      "metadata": {
        "id": "mY4H41Fc2zZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ns = np.array(train_data['n/s']) \n",
        "y_test_ns = np.array(test_data['n/s']) "
      ],
      "metadata": {
        "id": "WBbpoOni8dgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmXM2sfG8dg5"
      },
      "outputs": [],
      "source": [
        "## N/S분류를 위한 null 값 삭제 작업\n",
        "\n",
        "drop_train1_ns = [index for index, sentence in enumerate(y_train_ns) if sentence != 0 and sentence != 1 ]\n",
        "drop_test1_ns = [index for index, sentence in enumerate(y_test_ns) if sentence != 0 and sentence != 1 ]\n",
        "\n",
        "\n",
        "X_train_ns = np.delete(X_train_s, drop_train1_ns, axis=0)\n",
        "y_train_ns = np.delete(y_train_ns, drop_train1_ns, axis=0)\n",
        "\n",
        "X_test_ns = np.delete(X_test_s, drop_test1_ns, axis=0)\n",
        "y_test_ns = np.delete(y_test_ns, drop_test1_ns, axis=0)\n",
        "\n",
        "y_train_ns = y_train_ns.astype(float) \n",
        "y_test_ns = y_test_ns.astype(float)\n",
        "\n",
        "drop_train_ns = [index for index, sentence in enumerate(X_train_ns) if len(sentence) < 1]\n",
        "drop_test_ns = [index for index, sentence in enumerate(X_test_ns) if len(sentence) < 1]\n",
        "\n",
        "X_train_ns = np.delete(X_train_ns, drop_train_ns, axis=0)\n",
        "y_train_ns = np.delete(y_train_ns, drop_train_ns, axis=0)\n",
        "\n",
        "X_test_ns = np.delete(X_test_ns, drop_test_ns, axis=0)\n",
        "y_test_ns = np.delete(y_test_ns, drop_test_ns, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoRkMhdAQJDZ"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "below_threshold_len(max_len, X_train_ns)\n",
        "\n",
        "# 전체 데이터의 길이는 100으로 맞춘다.\n",
        "X_train_ns = pad_sequences(X_train_ns, maxlen = max_len)\n",
        "X_test_ns = pad_sequences(X_test_ns, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **F/T분류를 위한 null 값 삭제 작업**"
      ],
      "metadata": {
        "id": "xe10Wphg26Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_ft = np.array(train_data['f/t'])\n",
        "y_test_ft = np.array(test_data['f/t']) "
      ],
      "metadata": {
        "id": "59EaWKZRDvK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TnlU94PDvK9"
      },
      "outputs": [],
      "source": [
        "## F/T분류를 위한 null 값 삭제 작업\n",
        "drop_train1_ft = [index for index, sentence in enumerate(y_train_ft) if sentence != 0 and sentence != 1 ]\n",
        "drop_test1_ft = [index for index, sentence in enumerate(y_test_ft) if sentence != 0 and sentence != 1 ]\n",
        "\n",
        "\n",
        "X_train_ft = np.delete(X_train_s, drop_train1_ft, axis=0)\n",
        "y_train_ft = np.delete(y_train_ft, drop_train1_ft, axis=0)\n",
        "\n",
        "X_test_ft = np.delete(X_test_s, drop_test1_ft, axis=0)\n",
        "y_test_ft = np.delete(y_test_ft, drop_test1_ft, axis=0)\n",
        "\n",
        "y_train_ft = y_train_ft.astype(float) \n",
        "y_test_ft = y_test_ft.astype(float)\n",
        "\n",
        "drop_train_ft = [index for index, sentence in enumerate(X_train_ft) if len(sentence) < 1]\n",
        "drop_test_ft = [index for index, sentence in enumerate(X_test_ft) if len(sentence) < 1]\n",
        "\n",
        "X_train_ft = np.delete(X_train_ft, drop_train_ft, axis=0)\n",
        "y_train_ft = np.delete(y_train_ft, drop_train_ft, axis=0)\n",
        "\n",
        "X_test_ft = np.delete(X_test_ft, drop_test_ft, axis=0)\n",
        "y_test_ft = np.delete(y_test_ft, drop_test_ft, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfK3S5zCDvK9"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "below_threshold_len(max_len, X_train_ft)\n",
        "\n",
        "# 전체 데이터의 길이는 100으로 맞춘다.\n",
        "X_train_ft = pad_sequences(X_train_ft, maxlen = max_len)\n",
        "X_test_ft = pad_sequences(X_test_ft, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **P/J분류를 위한 null 값 삭제 작업**"
      ],
      "metadata": {
        "id": "ZH8_xxPv3BUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pj = np.array(train_data['p/j'])  \n",
        "y_test_pj = np.array(test_data['p/j'])"
      ],
      "metadata": {
        "id": "_iXMOnEQFzjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3FqtzB4Fzjz"
      },
      "outputs": [],
      "source": [
        "## P/J분류를 위한 null 값 삭제 작업\n",
        "\n",
        "drop_train1_pj = [index for index, sentence in enumerate(y_train_pj) if sentence != 0 and sentence != 1 ]\n",
        "drop_test1_pj = [index for index, sentence in enumerate(y_test_pj) if sentence != 0 and sentence != 1 ]\n",
        "\n",
        "\n",
        "X_train_pj = np.delete(X_train_s, drop_train1_pj, axis=0)\n",
        "y_train_pj = np.delete(y_train_pj, drop_train1_pj, axis=0)\n",
        "\n",
        "X_test_pj = np.delete(X_test_s, drop_test1_pj, axis=0)\n",
        "y_test_pj = np.delete(y_test_pj, drop_test1_pj, axis=0)\n",
        "\n",
        "y_train_pj = y_train_pj.astype(float) \n",
        "y_test_pj = y_test_pj.astype(float)\n",
        "\n",
        "drop_train_pj = [index for index, sentence in enumerate(X_train_pj) if len(sentence) < 1]\n",
        "drop_test_pj = [index for index, sentence in enumerate(X_test_pj) if len(sentence) < 1]\n",
        "\n",
        "X_train_pj = np.delete(X_train_pj, drop_train_pj, axis=0)\n",
        "y_train_pj = np.delete(y_train_pj, drop_train_pj, axis=0)\n",
        "\n",
        "X_test_pj = np.delete(X_test_pj, drop_test_pj, axis=0)\n",
        "y_test_pj = np.delete(y_test_pj, drop_test_pj, axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1MJYDyYFzjz"
      },
      "outputs": [],
      "source": [
        "max_len = 100\n",
        "below_threshold_len(max_len, X_train_pj)\n",
        "\n",
        "# 전체 데이터의 길이는 100으로 맞춘다.\n",
        "X_train_pj = pad_sequences(X_train_pj, maxlen = max_len)\n",
        "X_test_pj = pad_sequences(X_test_pj, maxlen = max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **E/I 분류 학습, 테스트**"
      ],
      "metadata": {
        "id": "i3ABGPOD3IGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kND2vtbN8ngH"
      },
      "outputs": [],
      "source": [
        "## E/I 분류 학습, 테스트\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "es_ei = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
        "mc_ei = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_ei = Sequential()\n",
        "model_ei.add(Embedding(vocab_size, 100))\n",
        "model_ei.add(LSTM(128, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "model_ei.add(Dropout(0.5))\n",
        "model_ei.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0005)))\n",
        "\n",
        "opt = Adam(learning_rate=5e-4)\n",
        "model_ei.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model_ei.fit(X_train_ei, y_train_ei, epochs=15, callbacks=[es_ei, mc_ei], batch_size=64, validation_split=0.2)\n",
        "loaded_model_ei = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model_ei.evaluate(X_test_ei, y_test_ei)[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N/S 분류 학습, 테스트**"
      ],
      "metadata": {
        "id": "4vhbLjwf3Q64"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOIokajvP1Fo"
      },
      "outputs": [],
      "source": [
        "## N/S 분류 학습, 테스트\n",
        "\n",
        "es_ns = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
        "mc_ns = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_ns = Sequential()\n",
        "model_ns.add(Embedding(vocab_size, 100))\n",
        "model_ns.add(LSTM(128, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "model_ns.add(Dropout(0.5))\n",
        "model_ns.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0005)))\n",
        "\n",
        "opt = Adam(learning_rate=5e-4)\n",
        "model_ns.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model_ns.fit(X_train_ns, y_train_ns, epochs=15, callbacks=[es_ns, mc_ns], batch_size=64, validation_split=0.2)\n",
        "loaded_model_ns = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model_ns.evaluate(X_test_ns, y_test_ns)[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **F/T 분류 학습, 테스트**"
      ],
      "metadata": {
        "id": "iLqWjdFX3Ubc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo6rL7fRSIuK"
      },
      "outputs": [],
      "source": [
        "## F/T 분류 학습, 테스트\n",
        "\n",
        "es_ft = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
        "mc_ft = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_ft = Sequential()\n",
        "model_ft.add(Embedding(vocab_size, 100))\n",
        "model_ft.add(LSTM(128, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "model_ft.add(Dropout(0.5))\n",
        "model_ft.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0005)))\n",
        "\n",
        "opt = Adam(learning_rate=5e-4)\n",
        "model_ft.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model_ft.fit(X_train_ft, y_train_ft, epochs=15, callbacks=[es_ft, mc_ft], batch_size=64, validation_split=0.2)\n",
        "loaded_model_ft = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model_ft.evaluate(X_test_ft, y_test_ft)[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **P/J 분류 학습, 테스트**"
      ],
      "metadata": {
        "id": "OkdtlPwk3XKo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqSDpcUySV3W"
      },
      "outputs": [],
      "source": [
        "## P/J 분류 학습, 테스트\n",
        "\n",
        "es_pj = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=4)\n",
        "mc_pj = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model_pj = Sequential()\n",
        "model_pj.add(Embedding(vocab_size, 100))\n",
        "model_pj.add(LSTM(128, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005), bias_regularizer=l2(0.0005)))\n",
        "model_pj.add(Dropout(0.5))\n",
        "model_pj.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0005)))\n",
        "\n",
        "opt = Adam(learning_rate=5e-4)\n",
        "model_pj.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model_pj.fit(X_train_pj, y_train_pj, epochs=15, callbacks=[es_pj, mc_pj], batch_size=64, validation_split=0.2)\n",
        "loaded_model_pj = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model_pj.evaluate(X_test_pj, y_test_pj)[1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ei = loaded_model_ei.evaluate(X_test_ei, y_test_ei)[1]\n",
        "pred_ns = loaded_model_ns.evaluate(X_test_ns, y_test_ns)[1]\n",
        "pred_ft = loaded_model_ft.evaluate(X_test_ft, y_test_ft)[1]\n",
        "pred_pj = loaded_model_pj.evaluate(X_test_pj, y_test_pj)[1]"
      ],
      "metadata": {
        "id": "gQmom_AC0hMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 각 지표의 정확도\n",
        "\n",
        "print(\"\\n E/I 테스트 정확도: %.4f\" % pred_ei)\n",
        "print(\"\\n N/S 테스트 정확도: %.4f\" % pred_ns)\n",
        "print(\"\\n F/T 테스트 정확도: %.4f\" % pred_ft)\n",
        "print(\"\\n P/J 테스트 정확도: %.4f\" % pred_pj)"
      ],
      "metadata": {
        "id": "FrNQQ23I0GVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "0515_lstm의 사본의 사본의 사본",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}